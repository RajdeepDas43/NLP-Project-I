{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration  \n\nfrom transformers import AdamW\nimport pandas as pd\nimport torch\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom torch.nn.utils.rnn import pad_sequence\n# from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n\npl.seed_everything(100)\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:13.810414Z",
          "iopub.execute_input": "2023-04-20T14:16:13.811352Z",
          "iopub.status.idle": "2023-04-20T14:16:40.687734Z",
          "shell.execute_reply.started": "2023-04-20T14:16:13.811309Z",
          "shell.execute_reply": "2023-04-20T14:16:40.686473Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Here **PyTorch-lightning** is used: PyTorch Lightning is a lightweight interface for PyTorch that simplifies the process of training deep learning models. It provides pre-built components and features for common tasks, making the code more modular and reusable. PyTorch Lightning also provides various features such as automatic checkpointing, distributed training, and multi-GPU training. It follows a strict design pattern and provides hooks and callbacks for customization. It is compatible with various hardware platforms such as CPUs and GPUs.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data = pd.read_csv(\"/kaggle/input/conversation-chatbot1/Conversation_Chatbot (1).csv\")\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:40.690106Z",
          "iopub.execute_input": "2023-04-20T14:16:40.690551Z",
          "iopub.status.idle": "2023-04-20T14:16:40.715600Z",
          "shell.execute_reply.started": "2023-04-20T14:16:40.690507Z",
          "shell.execute_reply": "2023-04-20T14:16:40.714627Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(\"No of rows:\" ,data.shape[0])",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:40.716993Z",
          "iopub.execute_input": "2023-04-20T14:16:40.717306Z",
          "iopub.status.idle": "2023-04-20T14:16:40.725035Z",
          "shell.execute_reply.started": "2023-04-20T14:16:40.717277Z",
          "shell.execute_reply": "2023-04-20T14:16:40.723765Z"
        },
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": "No of rows: 130\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "The task is to create a conversational model that can generate natural and engaging responses to a given input text. The model should be able to understand the context of the conversation and generate appropriate responses that are relevant to the topic and flow of the conversation. Additionally, the model should be able to handle open-ended conversations, where the topic can change dynamically, and maintain coherence throughout the conversation.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nINPUT_MAX_LEN = 128 #input length\nOUTPUT_MAX_LEN = 128 # output length\nTRAIN_BATCH_SIZE = 8 # batch size of training\nVAL_BATCH_SIZE = 2 # batch size for validation\nEPOCHS = 5 # number of epoch",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:40.728642Z",
          "iopub.execute_input": "2023-04-20T14:16:40.729449Z",
          "iopub.status.idle": "2023-04-20T14:16:40.849292Z",
          "shell.execute_reply.started": "2023-04-20T14:16:40.729412Z",
          "shell.execute_reply": "2023-04-20T14:16:40.848160Z"
        },
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "MODEL_NAME = \"t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:40.850919Z",
          "iopub.execute_input": "2023-04-20T14:16:40.851386Z",
          "iopub.status.idle": "2023-04-20T14:16:41.926065Z",
          "shell.execute_reply.started": "2023-04-20T14:16:40.851346Z",
          "shell.execute_reply": "2023-04-20T14:16:41.925086Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a03c7615634948f1be9cb2a126687631"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50d7c796716c44a8aa5868e148d65892"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Example of how T5 Tokenizer actually work.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "text = \"Hello, how are you today?\"    # assume the text that is to be tokenized \n\ninput_tokenize = tokenizer( \n             text,\n            add_special_tokens=True,        #Add Special tokens like [CLS] and [SEP]\n            max_length=128,\n            padding = 'max_length',         #for padding to max_length for equal sequence length\n            truncation = True,              #truncate the text if it is greater than max_length\n            return_attention_mask=True,     #will return attention mask\n            return_tensors=\"pt\"             #return tensor formate\n        )",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:41.927470Z",
          "iopub.execute_input": "2023-04-20T14:16:41.927967Z",
          "iopub.status.idle": "2023-04-20T14:16:41.936813Z",
          "shell.execute_reply.started": "2023-04-20T14:16:41.927920Z",
          "shell.execute_reply": "2023-04-20T14:16:41.935840Z"
        },
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(\"input_ids: \", input_tokenize['input_ids'].flatten())\nprint(\"-----------------------------------------------------------------------------\")\nprint(\"Attention Mask: \", input_tokenize['attention_mask'].flatten())",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:41.938556Z",
          "iopub.execute_input": "2023-04-20T14:16:41.939039Z",
          "iopub.status.idle": "2023-04-20T14:16:41.992122Z",
          "shell.execute_reply.started": "2023-04-20T14:16:41.938993Z",
          "shell.execute_reply": "2023-04-20T14:16:41.990278Z"
        },
        "trusted": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": "input_ids:  tensor([8774,    6,  149,   33,   25,  469,   58,    1,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0])\n-----------------------------------------------------------------------------\nAttention Mask:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0])\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class T5Dataset:\n    \n  def __init__(self,question,answer):   \n    \n    self.question = question\n    self.answer = answer\n    self.tokenizer = tokenizer\n    self.input_max_len = INPUT_MAX_LEN\n    self.output_max_len = OUTPUT_MAX_LEN\n  \n  def __len__(self):                      # This method retrives the number of item from the dataset\n    return len(self.question)\n\n  def __getitem__(self,item):             # This method retrieves the item at the specified index item. \n\n    question = str(self.question[item])\n    question = ''.join(question.split())\n\n    answer = str(self.answer[item])\n    answer = ''.join(answer.split())\n\n    input_tokenize = self.tokenizer(      \n            question,\n            add_special_tokens=True,\n            max_length=self.input_max_len,\n            padding = 'max_length',\n            truncation = True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n    output_tokenize = self.tokenizer(\n            answer,\n            add_special_tokens=True,\n            max_length=self.output_max_len,\n            padding = 'max_length',\n            truncation = True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n            \n        )\n    \n\n    input_ids = input_tokenize[\"input_ids\"].flatten()\n    attention_mask = input_tokenize[\"attention_mask\"].flatten()\n    labels = output_tokenize['input_ids'].flatten()\n\n    out = {\n            'question':question,      \n            'answer':answer,\n            'input_ids': input_ids,\n            'attention_mask':attention_mask,\n            'target':labels\n        }\n        \n    return out      ",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:41.994660Z",
          "iopub.execute_input": "2023-04-20T14:16:41.995132Z",
          "iopub.status.idle": "2023-04-20T14:16:42.006195Z",
          "shell.execute_reply.started": "2023-04-20T14:16:41.995096Z",
          "shell.execute_reply": "2023-04-20T14:16:42.004961Z"
        },
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class T5DataLoad(pl.LightningDataModule):\n    \n    def __init__(self,df_train,df_test):\n        super().__init__()\n        self.df_train = df_train\n        self.df_test = df_test\n        self.tokenizer = tokenizer\n        self.input_max_len = INPUT_MAX_LEN\n        self.out_max_len = OUTPUT_MAX_LEN\n    \n    def setup(self, stage=None):\n        \n        self.train_data = T5Dataset(\n            question = self.df_train.question.values,\n            answer = self.df_train.answer.values\n        )\n        \n        self.valid_data = T5Dataset(\n            question = self.df_test.question.values,\n            answer = self.df_test.answer.values\n        )\n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n         self.train_data,\n         batch_size= TRAIN_BATCH_SIZE,\n         shuffle=True, \n         num_workers=2\n        )\n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n        self.valid_data,\n        batch_size= VAL_BATCH_SIZE,\n        num_workers = 2\n        )",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:42.007994Z",
          "iopub.execute_input": "2023-04-20T14:16:42.008350Z",
          "iopub.status.idle": "2023-04-20T14:16:42.021291Z",
          "shell.execute_reply.started": "2023-04-20T14:16:42.008314Z",
          "shell.execute_reply": "2023-04-20T14:16:42.020192Z"
        },
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class T5Model(pl.LightningModule):\n    \n    def __init__(self):\n        super().__init__()\n        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, return_dict = True)\n\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        output = self.model(\n        input_ids=input_ids, \n        attention_mask=attention_mask, \n        labels=labels\n        )\n        return output.loss, output.logits\n    \n    def training_step(self, batch, batch_idx):\n\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels= batch[\"target\"]\n        loss, logits = self(input_ids , attention_mask, labels)\n\n        \n        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n\n        return {'loss': loss}\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        labels= batch[\"target\"]\n        loss, logits = self(input_ids, attention_mask, labels)\n\n        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n        \n        return {'val_loss': loss}\n\n    def configure_optimizers(self):\n        return AdamW(self.parameters(), lr=0.0001)",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:42.026321Z",
          "iopub.execute_input": "2023-04-20T14:16:42.026785Z",
          "iopub.status.idle": "2023-04-20T14:16:42.039427Z",
          "shell.execute_reply.started": "2023-04-20T14:16:42.026753Z",
          "shell.execute_reply": "2023-04-20T14:16:42.038431Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Final Training Step",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def run():\n    df_train, df_test = train_test_split(data,test_size = 0.2, random_state=100)\n    dataload = T5DataLoad(df_train,df_test)\n    dataload.setup()\n    device = DEVICE\n    model = T5Model()\n    model.to(device)\n    \n    checkpoint = ModelCheckpoint(\n        dirpath=\"/kaggle/working\",\n        filename='best-model',\n        save_top_k=2,\n        verbose=True,\n        monitor=\"val_loss\",\n        mode=\"min\"\n    )\n    trainer = pl.Trainer(\n        callbacks = checkpoint,\n        max_epochs= 1,\n        gpus=1,\n        accelerator=\"gpu\"\n    )\n    trainer.fit(model, dataload)\nrun()",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:16:42.040807Z",
          "iopub.execute_input": "2023-04-20T14:16:42.042055Z",
          "iopub.status.idle": "2023-04-20T14:17:24.831943Z",
          "shell.execute_reply.started": "2023-04-20T14:16:42.042018Z",
          "shell.execute_reply": "2023-04-20T14:17:24.830781Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/892M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f067e30297724f1d921cd751744a4f4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f18df739b05c4d99ac4f5e1c6185265d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Training: 0it [00:00, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0511e3b5d7a5484bafede2c71350a0d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Validation: 0it [00:00, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": ""
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "train_model = T5Model.load_from_checkpoint('/kaggle/working/best-model.ckpt')\ntrain_model.freeze()\n\ndef generate_question(question):\n\n    inputs_encoding =  tokenizer(\n        question,\n        add_special_tokens=True,\n        max_length= INPUT_MAX_LEN,\n        padding = 'max_length',\n        truncation='only_first',\n        return_attention_mask=True,\n        return_tensors=\"pt\"\n        )\n\n    \n    generate_ids = train_model.model.generate(\n        input_ids = inputs_encoding[\"input_ids\"],\n        attention_mask = inputs_encoding[\"attention_mask\"],\n        max_length = INPUT_MAX_LEN,\n        num_beams = 4,\n        num_return_sequences = 1,\n        no_repeat_ngram_size=2,\n        early_stopping=True,\n        )\n\n    preds = [\n        tokenizer.decode(gen_id,\n        skip_special_tokens=True, \n        clean_up_tokenization_spaces=True)\n        for gen_id in generate_ids\n    ]\n\n    return \"\".join(preds)\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:17:24.837683Z",
          "iopub.execute_input": "2023-04-20T14:17:24.838198Z",
          "iopub.status.idle": "2023-04-20T14:17:35.584736Z",
          "shell.execute_reply.started": "2023-04-20T14:17:24.838145Z",
          "shell.execute_reply": "2023-04-20T14:17:35.583555Z"
        },
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Model Evaluation",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "ques = \"hi, how are you doing?\"\nprint(\"Ques: \",ques)\nprint(\"BOT: \",generate_question(ques))",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:17:35.589022Z",
          "iopub.execute_input": "2023-04-20T14:17:35.591923Z",
          "iopub.status.idle": "2023-04-20T14:17:37.883565Z",
          "shell.execute_reply.started": "2023-04-20T14:17:35.591882Z",
          "shell.execute_reply": "2023-04-20T14:17:37.882274Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "name": "stdout",
          "text": "Ques:  hi, how are you doing?\nBOT:  hi, how are you doing?\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "ques = \"how's it going?\"\nprint(\"Ques: \",ques)\nprint(\"BOT: \",generate_question(ques))",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:17:37.885396Z",
          "iopub.execute_input": "2023-04-20T14:17:37.885831Z",
          "iopub.status.idle": "2023-04-20T14:17:38.876635Z",
          "shell.execute_reply.started": "2023-04-20T14:17:37.885790Z",
          "shell.execute_reply": "2023-04-20T14:17:38.875337Z"
        },
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "name": "stdout",
          "text": "Ques:  how's it going?\nBOT:  ?\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "ques = \"i heard that it's going to be warm this weekend.\"\nprint(\"Ques: \",ques)\nprint(\"BOT: \",generate_question(ques))",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-04-20T14:17:38.878486Z",
          "iopub.execute_input": "2023-04-20T14:17:38.878916Z",
          "iopub.status.idle": "2023-04-20T14:17:41.183442Z",
          "shell.execute_reply.started": "2023-04-20T14:17:38.878874Z",
          "shell.execute_reply": "2023-04-20T14:17:41.182145Z"
        },
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "name": "stdout",
          "text": "Ques:  i heard that it's going to be warm this weekend.\nBOT:  i heard that it's going to be warm this weekend.\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}